# Developer Research AI Agent

A robust, graph-based AI agent designed for deep technical research. It uses **LangGraph** for orchestration, **Ollama** for local LLM inference, **Qdrant** for persistent memory, and **DuckDuckGo/Tavily** for web search.

## ğŸš€ Features

- **Dual Mode Execution**:
  - **Quick Mode**: Direct LLM answers for simple queries.
  - **Deep Mode**: Iterative research loop with web search, gap analysis, and synthesis.
- **Persistent Memory**: Uses Qdrant to store user context and research history across sessions.
- **Guardrails**: Telemetry and budget tracking for query execution.
- **Intent Classification**: Automatically categorizes queries (Research, Bug Fix, Architecture, etc.).
- **Structured Output**: Generates detailed markdown reports saved to the `output/` directory.

## High-Level Data Flow

The system follows a multi-stage pipeline:
1.  **Ingestion & Guardrails**: Validates input and initializes state.
2.  **Contextualization**: Retrieves long-term memory.
3.  **Planning**: Determines intent and routing (Quick vs. Deep).
4.  **Execution**:
    -   *Quick Mode*: Immediate LLM response.
    -   *Deep Mode*: Iterative search and analysis loop.
5.  **Synthesis**: Compiles findings into a report.
6.  **Persistence**: Formats output and saves to disk/memory.

## Graph Diagram

```mermaid
graph TD
    Start([User Query]) --> Guard[Guard Layer]
    Guard --> Context[Context Retrieval]
    Context --> Classify[Intent Classification]
    
    Classify -- Unclear/Off-topic --> End([End])
    Classify -- Clear Intent --> Planner[Planner / Router]
    
    Planner -- Simple Query --> Quick[Quick Mode Executor]
    Planner -- Complex Query --> Deep[Deep Mode Orchestrator]
    
    subgraph "Deep Research Loop"
        Deep --> Gap[Gap Analysis]
        Gap -- "Confidence < 0.8" --> Deep
        Gap -- "Confidence >= 0.8" --> Synthesize[Structured Synthesis]
    end
    
    Quick --> Formatter[Output Formatter]
    Synthesize --> Formatter
    
    Formatter --> Disk[(Save to Disk)]
    Formatter --> Memory[(Save to Qdrant)]
    Formatter --> End
```


## ğŸ“‚ Project Structure

```bash
.
â”œâ”€â”€ main.py                     # Entry point for the agent
â”œâ”€â”€ config.py                   # Configuration settings (Model, API keys)
â”œâ”€â”€ state.py                    # Graph state definition (TypedDict)
â”œâ”€â”€ memory.py                   # Qdrant integration for persistent memory
â”œâ”€â”€ graph/                      # Core logic nodes (LangGraph)
â”‚   â”œâ”€â”€ nodes_pre.py            # Guard, Context, Intent Classification
â”‚   â”œâ”€â”€ nodes_exec.py           # Planner, Quick/Deep Execution modes
â”‚   â”œâ”€â”€ nodes_post.py           # Output formatting and saving
â”‚   â”œâ”€â”€ builder.py              # Graph construction logic (optional refactor)
â”‚   â””â”€â”€ routes.py               # Conditional edge logic
â”œâ”€â”€ output/                     # Generated markdown reports
â”œâ”€â”€ qdrant_db/                  # Local Vector DB storage
â”œâ”€â”€ tools/                      # External tool definitions
â”‚   â”œâ”€â”€ search_tools.py
â”‚   â””â”€â”€ memory_tools.py
â”œâ”€â”€ prompts/                    # LLM Prompt templates
â”œâ”€â”€ test_research_agent.py      # End-to-end automated test script
â”œâ”€â”€ test_memory.py              # Memory persistence test script
â””â”€â”€ requirements.txt            # Python dependencies
```

## ï¿½ğŸ“¦ Installation

### Prerequisites
- Python 3.10+
- [Ollama](https://ollama.com/) running locally.
- Recommended Model: `ministral-3` (or configure your own in `config.py`).

### Setup

1.  **Clone the repository** (if applicable) or navigate to the project directory.

2.  **Create a Virtual Environment**:
    ```bash
    python3 -m venv .venv
    source .venv/bin/activate
    ```

3.  **Install Dependencies**:
    ```bash
    pip install -r requirements.txt
    ```

4.  **Environment Variables**:
    Create a `.env` file in the root directory:
    ```env
    # Optional: For better search results
    TAVILY_API_KEY=your_api_key_here
    ```

## ğŸƒ Usage

### Streamlit Web Interface (Recommended)
Launch the professional web interface:
```bash
streamlit run app.py
```
Features:
- ğŸ’¬ Interactive chat interface
- ğŸ“Š Real-time progress tracking
- ğŸ¯ Session history
- ğŸ“ˆ Token usage statistics
- ğŸ¨ Responsive, professional design

### Interactive CLI Mode
Run the main script to start the interactive CL agent:
```bash
python main.py
```
Type your query when prompted. Type `exit` to quit.

### Automated Testing
Run the comprehensive test script to verify the full workflow:
```bash
python test_research_agent.py
```
This script runs a complex query ("Mamba vs Transformer") and asserts that all graph nodes execute correctly.

### Check Memory
Verify memory persistence:
```bash
python test_memory.py
```

## ğŸ“‚ Output

- **Reports**: Saved as `research_report_YYYYMMDD_HHMMSS.md` in the `output/` directory.
- **Memory**: Stored locally in `qdrant_db/`.

## âš™ï¸ Configuration

Modify `config.py` to change:
-   `MODEL_NAME`: The Ollama model to use.
-   `MAX_ITERATIONS_DEEP_MODE`: Research depth.
-   `CONFIDENCE_THRESHOLD`: When to stop researching.

